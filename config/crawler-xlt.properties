## The crawler's reported user name. Might be important to parse robots.txt correctly.
## If property is not set, the default value is 'XLT-Bot'
crawler.user.name = XLT Bot/1.0

## The following properties define where the crawler starts. That might be the globally configured start page (homepage),
## an already known (crawled) page, or a page listed in the sitemap.
## All possibilities can be weighted. The weight defines how likely the crawler starts with this option. If no option
## is set at all, the global start URL is taken as start page. Values 'HOMEPAGE', 'KNOWN_PAGE', and 'SITEMAP' are supported only.
## All other values will be ignored and 'HOMEPAGE' used instead.
crawler.startAt.1 = HOMEPAGE
crawler.startAt.1.weight = 20

crawler.startAt.2 = KNOWN_PAGE
crawler.startAt.2.weight = 70

crawler.startAt.3 = SITEMAP
crawler.startAt.3.weight = 10

## The crawler will follow the rules of robots.txt, meta tags, and tagged links
## by default, which is also the behavior in case the property is not set at all.
## This behavior can be switched off if desired.
crawler.rule.robots = true
crawler.rule.meta = true
crawler.rule.linkrel = true

## Probability to force the crawler to hit a non-existing page
crawler.miss.probability = 10

## Expected status code in case of page miss. This might be a common return code as well as a regular expression.
## Example: 200
## Example: 404
## Example: (200|404)
## Example: \\d{3}
## To deactivate status code checking, disable the property or clear its value.
crawler.miss.statusCode = 404

## Expected text contained on as page-miss page. The value is interpreted as regular expression
## Example: URL not found
## Example: Sorry.*?not found
## To deactivate status code checking, disable the property or clear its value.
crawler.miss.expectedTextRegEx = URL Not Found

## URL filter. Separate several URLs by space.
## Default: value of 'com.xceptance.xlt.http.filter.include'
#crawler.includeURLs =
crawler.excludeURLs = ^mailto: ^# \\.jpg$ \\.png$ \\.pdf$ \\.txt$ \\.xml$

## Max nurmber of pages to crawl. Stop crawling if that number is reached. If value is zero or below, no page will be visited.
crawler.maxPages = 100

## Time to run is taken from random value between MIN and MAX. Stop when crawler runtime is reached. Runtime limit
## must be 1 sec or higher.
## Number AND unit required, such as 10m, 30s, 1h OR combination like 3h2m1s.
## Ranges can be
##   (from-to)          10m - 1h
##   (lower limit only) 10m -
##   (upper limit only)     - 10m
## The dash is mandatory.
## If no lower limit is given, 0 is assumed.
## If no upper limit is given, Integer.MAX_VALUE is assumed.
crawler.runtime.range = 20s - 5m

## Max depth of recursion
## Default: 1 (don't crawl into depth at all)
crawler.maxDepth = 2

## Add some dynamic parameter to the URLs to crawl to disable server side caching
crawler.noCache = false

## Drop Session
## * always
## * never
## * every <INTERVAL><UNIT>
## Unit might by a time unit such as h,m,s (meaning hour, minute, and second)
## OR unit might be "p" for pages
##
## Examples:
## crawler.dropSession = always
## crawler.dropSession = never
## crawler.dropSession = every 1h30m
## crawler.dropSession = every 3p
crawler.dropSession = never

## Text Filters
## Add as many text filters as needed. Take care the text filter ID
## is unique for the current filter. The ID doesn't need to be a number
## necessarily.

## Disallowed texts
#crawler.disallowText.1 =
#crawler.disallowText.2 =
#crawler.disallowText.3 =

## Required texts
#crawler.requireText.1 =
#crawler.requireText.2 =
#crawler.requireText.3 =